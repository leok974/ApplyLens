name: Analytics ML Training & Forecasting

on:
  schedule:
    # Weekly model training (Sundays at 4:00 AM UTC)
    - cron: '0 4 * * 0'
    # Daily forecasting (every day at 4:45 AM UTC)
    - cron: '45 4 * * *'
  workflow_dispatch:
    inputs:
      job_type:
        description: 'Job type to run'
        required: true
        default: 'forecast'
        type: choice
        options:
          - train
          - forecast
          - both

env:
  PYTHON_VERSION: '3.11'
  DBT_VERSION: '1.7.0'

jobs:
  train-models:
    name: Train ARIMA Models
    runs-on: ubuntu-latest
    # Only run training on Sunday schedule or manual trigger with train/both
    if: |
      (github.event_name == 'schedule' && github.event.schedule == '0 4 * * 0') ||
      (github.event_name == 'workflow_dispatch' && (github.event.inputs.job_type == 'train' || github.event.inputs.job_type == 'both'))
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-bigquery
        run: |
          pip install dbt-bigquery==${{ env.DBT_VERSION }}

      - name: Authenticate to GCP
        id: auth-train
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.BQ_SA_JSON }}

      - name: Sanity check BigQuery secrets
        env:
          BQ_PROJECT: ${{ secrets.BQ_PROJECT }}
          BQ_SA_JSON: ${{ secrets.BQ_SA_JSON }}
        run: |
          echo "BQ_PROJECT length: ${#BQ_PROJECT}"
          echo "BQ_SA_JSON length: ${#BQ_SA_JSON}"
          if [ -z "$BQ_PROJECT" ] || [ -z "$BQ_SA_JSON" ]; then
            echo "❌ Missing BigQuery secrets!"
            exit 1
          fi
          echo "✅ BigQuery secrets are configured"

      - name: Train ARIMA models
        working-directory: analytics/dbt
        env:
          BQ_PROJECT: ${{ secrets.BQ_PROJECT }}
          BQ_SA_JSON: ${{ secrets.BQ_SA_JSON }}
        run: |
          dbt run --select ml:m_* --target prod --profiles-dir .
          echo "✅ Model training complete"

  forecast-and-detect:
    name: Generate Forecasts & Detect Anomalies
    runs-on: ubuntu-latest
    # Run on daily schedule or manual trigger with forecast/both
    # Also run after training job completes
    needs: [train-models]
    if: |
      always() &&
      (needs.train-models.result == 'success' || needs.train-models.result == 'skipped') &&
      (
        (github.event_name == 'schedule' && github.event.schedule == '45 4 * * *') ||
        (github.event_name == 'workflow_dispatch' && (github.event.inputs.job_type == 'forecast' || github.event.inputs.job_type == 'both')) ||
        needs.train-models.result == 'success'
      )
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install dbt-bigquery==${{ env.DBT_VERSION }}
          pip install google-cloud-bigquery elasticsearch

      - name: Authenticate to GCP
        id: auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.BQ_SA_JSON }}

      - name: Sanity check BigQuery secrets
        env:
          BQ_PROJECT: ${{ secrets.BQ_PROJECT }}
          BQ_SA_JSON: ${{ secrets.BQ_SA_JSON }}
        run: |
          echo "BQ_PROJECT length: ${#BQ_PROJECT}"
          echo "BQ_SA_JSON length: ${#BQ_SA_JSON}"
          if [ -z "$BQ_PROJECT" ] || [ -z "$BQ_SA_JSON" ]; then
            echo "❌ Missing BigQuery secrets!"
            exit 1
          fi
          echo "✅ BigQuery secrets are configured"

      - name: Generate forecasts
        working-directory: analytics/dbt
        env:
          BQ_PROJECT: ${{ secrets.BQ_PROJECT }}
          BQ_SA_JSON: ${{ secrets.BQ_SA_JSON }}
        run: |
          dbt run --select ml:pred_* --target prod --profiles-dir .
          echo "✅ Forecasts generated"

      - name: Detect anomalies
        working-directory: analytics/dbt
        env:
          BQ_PROJECT: ${{ secrets.BQ_PROJECT }}
          BQ_SA_JSON: ${{ secrets.BQ_SA_JSON }}
        run: |
          dbt run --select ml:anomaly_detection --target prod --profiles-dir .
          echo "✅ Anomalies detected"

      - name: Export anomalies to Elasticsearch
        env:
          BQ_PROJECT: ${{ secrets.BQ_PROJECT }}
          ES_URL: ${{ secrets.ES_URL }}
          GOOGLE_APPLICATION_CREDENTIALS: ${{ steps.auth.outputs.credentials_file_path }}
        run: |
          python analytics/export/export_anomalies_to_es.py
