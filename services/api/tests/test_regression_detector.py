"""Tests for regression detector and auto-rollback.

Tests:
- Metrics aggregation from audit logs
- Regression detection (quality, latency, cost)
- Automatic rollback triggering
- Threshold enforcement
"""

import pytest
from datetime import datetime, timedelta
from unittest.mock import Mock, MagicMock, patch

from app.guard.regression_detector import (
    RegressionDetector,
    MetricsStore,
    REGRESS_MAX_QUALITY_DROP,
    REGRESS_MAX_LATENCY_P95_MS,
    REGRESS_MAX_COST_CENTS,
    REGRESS_MIN_SAMPLE
)
from app.models import AgentAuditLog


class TestMetricsStore:
    """Test metrics aggregation."""
    
    @pytest.fixture
    def mock_db(self):
        """Mock database session."""
        return Mock()
    
    def test_window_stats_separates_v1_v2(self, mock_db):
        """Should correctly separate V1 and V2 runs."""
        store = MetricsStore(mock_db)
        
        # Mock runs
        v1_run = Mock(spec=AgentAuditLog)
        v1_run.status = "succeeded"
        v1_run.plan = {"planner_meta": {"selected": "v1"}}
        v1_run.duration_ms = 500.0
        
        v2_run = Mock(spec=AgentAuditLog)
        v2_run.status = "succeeded"
        v2_run.plan = {"planner_meta": {"selected": "v2"}}
        v2_run.duration_ms = 1200.0
        
        mock_db.query.return_value.filter.return_value.order_by.return_value.limit.return_value.all.return_value = [v1_run, v2_run]
        
        stats = store.window_stats(window_runs=10)
        
        assert stats["v1"]["samples"] == 1
        assert stats["v2"]["samples"] == 1
    
    def test_window_stats_computes_aggregates(self, mock_db):
        """Should compute quality, latency, cost aggregates."""
        store = MetricsStore(mock_db)
        
        # Mock V2 runs with varying latencies
        runs = []
        for latency in [100, 200, 500, 1000, 2000]:
            run = Mock(spec=AgentAuditLog)
            run.status = "succeeded"
            run.plan = {"planner_meta": {"selected": "v2"}}
            run.duration_ms = float(latency)
            runs.append(run)
        
        mock_db.query.return_value.filter.return_value.order_by.return_value.limit.return_value.all.return_value = runs
        
        stats = store.window_stats(window_runs=10)
        
        assert stats["v2"]["samples"] == 5
        assert stats["v2"]["latency_p95_ms"] > 1000  # p95 should be high
    
    def test_window_stats_handles_no_runs(self, mock_db):
        """Should handle case with no runs."""
        store = MetricsStore(mock_db)
        
        mock_db.query.return_value.filter.return_value.order_by.return_value.limit.return_value.all.return_value = []
        
        stats = store.window_stats(window_runs=10)
        
        assert stats["v1"]["samples"] == 0
        assert stats["v2"]["samples"] == 0


class TestRegressionDetector:
    """Test regression detection logic."""
    
    @pytest.fixture
    def mock_store(self):
        """Mock metrics store."""
        return Mock(spec=MetricsStore)
    
    @pytest.fixture
    def mock_settings_dao(self):
        """Mock runtime settings DAO."""
        return Mock()
    
    def test_insufficient_sample_no_action(self, mock_store, mock_settings_dao):
        """Should not evaluate with insufficient V2 samples."""
        detector = RegressionDetector(mock_store, mock_settings_dao)
        
        # Mock insufficient samples
        mock_store.window_stats.return_value = {
            "v1": {"samples": 100, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 1.0},
            "v2": {"samples": 10, "quality": 90.0, "latency_p95_ms": 600, "cost_cents": 1.2}  # < 30 samples
        }
        
        result = detector.evaluate()
        
        assert result["action"] == "none"
        assert result["reason"] == "insufficient_sample"
        assert not mock_settings_dao.update.called
    
    def test_quality_regression_triggers_rollback(self, mock_store, mock_settings_dao):
        """Should detect quality regression and trigger rollback."""
        detector = RegressionDetector(mock_store, mock_settings_dao)
        
        # Mock quality drop > 5 points
        mock_store.window_stats.return_value = {
            "v1": {"samples": 50, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 1.0},
            "v2": {"samples": 50, "quality": 85.0, "latency_p95_ms": 500, "cost_cents": 1.0}  # 10 point drop
        }
        
        result = detector.evaluate()
        
        assert result["action"] == "rollback"
        assert result["reason"] == "regression_detected"
        assert len(result["breaches"]) > 0
        assert "quality" in result["breaches"][0]
        
        # Should trigger settings update
        assert mock_settings_dao.update.called
        call_args = mock_settings_dao.update.call_args[0][0]
        assert call_args["planner_kill_switch"] is True
        assert call_args["planner_canary_pct"] == 0.0
    
    def test_latency_regression_triggers_rollback(self, mock_store, mock_settings_dao):
        """Should detect latency regression and trigger rollback."""
        detector = RegressionDetector(mock_store, mock_settings_dao)
        
        # Mock latency > 1600ms
        mock_store.window_stats.return_value = {
            "v1": {"samples": 50, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 1.0},
            "v2": {"samples": 50, "quality": 95.0, "latency_p95_ms": 2000, "cost_cents": 1.0}  # > 1600ms
        }
        
        result = detector.evaluate()
        
        assert result["action"] == "rollback"
        assert "latency" in result["breaches"][0]
    
    def test_cost_regression_triggers_rollback(self, mock_store, mock_settings_dao):
        """Should detect cost regression and trigger rollback."""
        detector = RegressionDetector(mock_store, mock_settings_dao)
        
        # Mock cost > 3.0 cents
        mock_store.window_stats.return_value = {
            "v1": {"samples": 50, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 1.0},
            "v2": {"samples": 50, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 4.0}  # > 3.0Â¢
        }
        
        result = detector.evaluate()
        
        assert result["action"] == "rollback"
        assert "cost" in result["breaches"][0]
    
    def test_multiple_breaches(self, mock_store, mock_settings_dao):
        """Should detect multiple simultaneous breaches."""
        detector = RegressionDetector(mock_store, mock_settings_dao)
        
        # Mock all three breaches
        mock_store.window_stats.return_value = {
            "v1": {"samples": 50, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 1.0},
            "v2": {"samples": 50, "quality": 80.0, "latency_p95_ms": 2000, "cost_cents": 5.0}
        }
        
        result = detector.evaluate()
        
        assert result["action"] == "rollback"
        assert len(result["breaches"]) == 3  # All three should be detected
    
    def test_no_regression_ok_status(self, mock_store, mock_settings_dao):
        """Should return OK when no regressions detected."""
        detector = RegressionDetector(mock_store, mock_settings_dao)
        
        # Mock good stats (no breaches)
        mock_store.window_stats.return_value = {
            "v1": {"samples": 50, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 1.0},
            "v2": {"samples": 50, "quality": 94.0, "latency_p95_ms": 550, "cost_cents": 1.1}  # All within bounds
        }
        
        result = detector.evaluate()
        
        assert result["action"] == "ok"
        assert result["reason"] == "within_thresholds"
        assert not mock_settings_dao.update.called
    
    def test_exact_threshold_no_rollback(self, mock_store, mock_settings_dao):
        """Should not rollback when exactly at threshold."""
        detector = RegressionDetector(mock_store, mock_settings_dao)
        
        # Mock exactly at thresholds (should not breach)
        mock_store.window_stats.return_value = {
            "v1": {"samples": 50, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 1.0},
            "v2": {"samples": 50, "quality": 90.0, "latency_p95_ms": 1600, "cost_cents": 3.0}  # Exactly at limits
        }
        
        result = detector.evaluate()
        
        # Should be OK (thresholds use >, not >=)
        assert result["action"] == "ok"


class TestRollbackIntegration:
    """Integration tests for rollback workflow."""
    
    def test_rollback_sets_correct_values(self):
        """Rollback should set kill_switch=True and canary_pct=0."""
        mock_settings_dao = Mock()
        mock_store = Mock(spec=MetricsStore)
        
        # Mock severe regression
        mock_store.window_stats.return_value = {
            "v1": {"samples": 50, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 1.0},
            "v2": {"samples": 50, "quality": 70.0, "latency_p95_ms": 3000, "cost_cents": 10.0}
        }
        
        detector = RegressionDetector(mock_store, mock_settings_dao)
        result = detector.evaluate()
        
        assert result["action"] == "rollback"
        
        # Verify settings update
        call_args = mock_settings_dao.update.call_args
        updates = call_args[0][0]
        assert updates["planner_kill_switch"] is True
        assert updates["planner_canary_pct"] == 0.0
        
        # Verify audit metadata
        assert call_args[1]["updated_by"] == "regression_detector"
        assert "auto_rollback" in call_args[1]["reason"]
    
    def test_rollback_includes_breach_details(self):
        """Rollback reason should include breach details."""
        mock_settings_dao = Mock()
        mock_store = Mock(spec=MetricsStore)
        
        mock_store.window_stats.return_value = {
            "v1": {"samples": 50, "quality": 95.0, "latency_p95_ms": 500, "cost_cents": 1.0},
            "v2": {"samples": 50, "quality": 80.0, "latency_p95_ms": 2500, "cost_cents": 1.0}
        }
        
        detector = RegressionDetector(mock_store, mock_settings_dao)
        detector.evaluate()
        
        # Check reason contains breach info
        call_args = mock_settings_dao.update.call_args
        reason = call_args[1]["reason"]
        assert "quality" in reason or "latency" in reason
